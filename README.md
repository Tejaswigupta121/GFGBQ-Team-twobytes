# GFGBQ-Team-twobytes
Repository for twobytes - Vibe Coding Hackathon
# Problem Statement
PS 03: AI Hallucination and Citation Verification System

Generative AI models are widely used for research, learning, and decision making. However, these models often generate factually incorrect information presented with high confidence. A critical extension of this problem is the creation of fake citations, non-existent references, and broken links, which appear legitimate but cannot be verified. This makes it difficult for users to trust AI-generated content and may lead to misinformation, legal risks, and ethical concerns.

Build a system that can detect, flag, and verify factual claims and citations generated by AI models, helping users distinguish between reliable and unreliable AI-produced information.



---

## 2ï¸âƒ£ Project Name

**HalluciNOT â€“ AI Hallucination & Citation Verifier**

---

## 3ï¸âƒ£ Team Name

**Team twobytes**

---

## 4ï¸âƒ£ Deployed Link (optional)

ğŸ”— *Not deployed (local Streamlit application)*
*(Can be updated if deployed later)*

---

## 5ï¸âƒ£ 2-Minute Demonstration Video Link

ğŸ¥ *[Add your Google Drive / YouTube link here]*

---

## 6ï¸âƒ£ PPT Link

ğŸ“Šhttps://docs.google.com/presentation/d/17D-ImgNdI-QYHA3a9GS-ZrHZ1vcdcor4/edit?usp=sharing&ouid=106658113168494542941&rtpof=true&sd=true

---

---

# ğŸ“„ Project Overview

**HalluciNOT** is an AI-powered system that detects hallucinations in AI-generated text by:

* Extracting factual claims
* Retrieving real-world evidence from indexed datasets (Wikipedia + internal corpus)
* Verifying claims using Natural Language Inference (BART-MNLI)
* Detecting fake or unverifiable citations
* Computing a transparent **Trust Score**
* Generating downloadable **JSON & PDF verification reports**

The goal is to **increase trust, accountability, and explainability** in AI-generated content.

---

# ğŸ§  System Architecture

**Pipeline Flow:**

```
AI Text Input
   â†“
Claim Extraction
   â†“
Evidence Retrieval (FAISS + Wikipedia)
   â†“
Claim Verification (MNLI)
   â†“
Trust Score Computation
   â†“
Evidence-backed Verdict + Reports
```

---

# ğŸ› ï¸ Tech Stack

* **Python**
* **Streamlit** â€“ Interactive UI
* **SentenceTransformers** â€“ Semantic embeddings
* **FAISS** â€“ Vector similarity search
* **BART-Large-MNLI** â€“ Claim verification
* **Wikipedia Dataset** â€“ Real-world grounding
* **ReportLab** â€“ PDF report generation

---

# âš™ï¸ Setup & Installation Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/ByteQuest-2025/GFGBQ-Team-twobytes.git
cd GFGBQ-Team-twobytes
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows
# OR
source venv/bin/activate  # Linux/Mac
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Build Evidence Index (Required Once)

```bash
python build_index.py
```

This step:

* Ingests internal + Wikipedia data
* Creates FAISS vector index

---

# â–¶ï¸ Usage Instructions

### Run the Application

```bash
streamlit run app.py
```

### How to Use:

1. Paste AI-generated text
2. (Optional) Load demo sample
3. Click **Verify**
4. View:

   * Claim-wise verdicts
   * Evidence sources
   * Confidence scores
   * Trust Score summary
5. Download:

   * JSON verification report
   * PDF verification report

---

# ğŸ“Š Trust Score Explanation

```
Trust Score = (Supported Claims / Total Claims) Ã— 100
```

Color-coded interpretation:

* ğŸŸ¢ **High Trust** (â‰¥ 70%)
* ğŸŸ¡ **Moderate Trust** (40â€“69%)
* ğŸ”´ **Low Trust** (< 40%)

---

# ğŸ§ª Key Features

âœ… Claim-level verification
âœ… Wikipedia-backed evidence retrieval
âœ… Numeric-claim safety checks
âœ… Fake citation detection
âœ… Transparent explanations
âœ… PDF & JSON report export
âœ… Clean, judge-friendly UI
âœ… HalluciNOT branding & logo

---

## ğŸ“¸ Relevant Screenshots

### ğŸ  Home UI
![Home UI](assets/Home.png)

### ğŸ“Š Trust Score Summary
![Trust Score Summary](assets/trust_report.png)

### ğŸ“„ PDF Report
![PDF Report](assets/pdf_report.png)

---


# ğŸ Conclusion

**HalluciNOT** addresses one of the most critical problems in modern AI â€” **hallucinated facts and fake citations** â€” by combining semantic retrieval, natural language inference, and transparent scoring into a single, user-friendly verification system.

