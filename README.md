# GFGBQ-Team-twobytes
Repository for twobytes - Vibe Coding Hackathon
# Problem Statement
PS 03: AI Hallucination and Citation Verification System

Generative AI models are widely used for research, learning, and decision making. However, these models often generate factually incorrect information presented with high confidence. A critical extension of this problem is the creation of fake citations, non-existent references, and broken links, which appear legitimate but cannot be verified. This makes it difficult for users to trust AI-generated content and may lead to misinformation, legal risks, and ethical concerns.

Build a system that can detect, flag, and verify factual claims and citations generated by AI models, helping users distinguish between reliable and unreliable AI-produced information.
# Project Name
HalluciNOT

# Team Name
TwoBytes

# Deployed Link


# 2-minute Demonstration Video Link


# PPT Link

# Solution Overview

We built an end-to-end AI hallucination detection system that:

Extracts verifiable claims from text

Retrieves relevant evidence from trusted datasets

Verifies claims using Natural Language Inference (NLI)

Detects and validates citations

Generates trust scores and human-readable explanations

Provides a clean, interactive Streamlit UI
